\section{Diskrete fordelinger}
\begin{defn} [Indikator] %2.12. Bernoulli random variable
Lad $A$ være en hændelse. Den stokastiske variabel $\mathbb{1}_A$ er en indikator af $A$, hvis
\begin{align*}
    \mathbb{1}_A=\begin{cases} 
      1 & \text{hvis } A \text{ opstår} \\
      0 & \text{ellers}
   \end{cases}
\end{align*}
\end{defn}

Lad $A$ være en hændelse i et forsøg, der gentages $n$ uafhængige gange. Antallet af forsøg, der skal udføres for, at $A$ opstår første gang, noters så med $X$, som tager udfaldsrummet $1,2,\ldots$. Hændelsen $\{X=k\}$ betyder, at vi har $k-1$ fejslag, før vi får succes ved det $k$'te forsøg. Grundet uafhængighed, så er sandsynligheden for $\{X=k\}$ givet ved
\begin{align*}
    \lambda(1-\lambda)(1-\lambda)\cdots (1-\lambda) =\lambda(1-\lambda)^{k-1}
\end{align*}
hvor $\lambda \in [0, 1]$ er sandsynligheden for succes.
\begin{defn} \label{def:2.14} %Def 2.14
Hvis $X$ er antallet af forsøg indtil en hændelse $A$ med sandsynlighed $\lambda>0$ opstår i et uafhængigt eksperiment som gentages, så har $X$ pmf 
\begin{align*}
    p(k)=\lambda(1-\lambda)^{k-1}, \quad k=1,2,\ldots 
\end{align*}
og vi siger at $X$ har en geometrisk fordeling med succes sandsynlighed $\lambda$, som noteres $X\sim\geom(\lambda)$.
\end{defn}

Hvis der i stedet for kigges på $Y$, som er antallet af forsøg, strengt før hændelsen $A$ opstår, så er pmf
\begin{align*}
    p(k)=\lambda(1-\lambda)^{k}, \quad k=0,1,2,\ldots 
\end{align*}
hvilket svare til en geometrisk fordeling med $k$ startende i $0$. Det gælder også, at $Y=X-1$.

\begin{exmp}
En terning kastes indtil den første sekser bliver slået, defineres hændelserne $A_k = \{\text{det } k \text{'te kast er en 6'er}\}$, så er $A_1, A_2, \ldots$ uafhængige, se eksempel \ref{exmp:uafhængelighed}. Lad $X$ være antallet af terninger kastet før den første sekser bliver slået, så er $X \sim \geom(\tfrac{1}{6})$, da der er tale om et stokastisk forsøg, som gentages $n$ uafhængige gange. Hvad er sandsynligheden for, at den første sekser kommer ved $10$'ene slag? Vi har 
\begin{equation*}
    p(10) = \frac{1}{6} \left(1-\frac{1}{6}\right)^{10 - 1} \approx 0.032.
\end{equation*}
Altså en sandsynlighed på omkring 3\%.
\end{exmp}


\begin{prop} %2.18 
\label{prop:geomEgenskaber}
Hvis $X \sim \geom (p)$, så gælder 
\begin{align*}
    E[X] = \frac{1}{p}, \text{ og } \Var[X] = \frac{1-p}{p^2}
\end{align*}
\end{prop}

\begin{proof}
\begin{align*}
    E[X]&=\sum_{k=1}^\infty kp(1-p)^{k-1} =p\sum_{k=0}^\infty k(1-p)^{k-1}=p\frac{1}{1-(1-p)^2}=\frac{1}{p}
\end{align*}
% Ud fra proposition \ref{prop:2.9} kan middelværdien af $X$ opskrives som:
% \begin{align*}
%   E[X] &= \sum_{n=0}^{\infty} P(X > n) =\sum_{n=0}^\infty(1-p)^n=\frac{1}{1-(1-p)}=\frac{1}{p}.
% \end{align*}
% Da $\sum_{n=0}^\infty x^n=\frac{1}{1-x}$ for $0 < x < 1$, kan man finde den afledte og dobbeltafledte:
% \begin{align} \label{dobleafledt}
%     E'[X]&=\sum_{n=1}^{\infty}nx^{n-1}=\frac{1}{(1-x)^2} \nonumber \\ %bemærk man tager summe fra 1, fordi at afledte af 0 bliver 0.
%     E''[X]&=\sum_{n=2}^{\infty}n(n-1)x^{n-2}=\frac{2}{(1-x)^3} %bemærk man tager summe fra 2, fordi at afledte af 1 bliver 0.
% \end{align}

Man kan opskrive $\Var[X] =E[X^2]-E[X]^2$, jævnfør korollar \ref{cor:VariansIForholdTilForventedVærdi}\\
Ud fra lemma \ref{lem:middelværdiAfX2} er $E[X^2]$ givet ved:
\begin{align*}
E[X^2]&=E[X]+E[X(X-1)]\\
    &=\frac{1}{p}+\sum_{k=1}^{\infty}k(k-1)p(1-p)^{k-1}\\
    &=\frac{1}{p} + p(1-p)\sum_{k=0}^{\infty}k(k-1)(1-p)^{k-2}\\
    &=\frac{1}{p}+p(1-p)\frac{2}{(1-(1-p))^3}\\
    &=\frac{2-p}{p^2}
\end{align*}
% Ved at benytte ligning \eqref{dobleafledt}, opnås:
% \begin{align*}
%     E[X^2] = \frac{1}{p} + p(1-p)\frac{2}{(1-(1-p))^3}= \frac{1}{p} + \frac{2(1-p)}{p^2}%som kommer fra den dobbeltafledte, hvor x svare til 1-p og n svare til k
% \end{align*}
og derved er:
\begin{align*}
     \Var[X]&=E[X^2]-E[X]^2
 %    &=E[X(X-1)]+E[X]-E[X]^2\\
     =\frac{2-p}{p^2}-\left(\frac{1}{p}\right)^2=\frac{1-p}{p^2}
\end{align*}
% Heraf fås $\Var[X] = \dfrac{1 - p}{p^2}$.
\end{proof}


Et andet eksempel på en diskrete fordeling er Poissons. Denne fordeling kan benyttes til at beskrive antallet af stokastiske hændelser, der bliver empirisk observeret. Eksempelvis antallet af jordskælv over et årti.
%Def 2.15
\begin{defn}\label{def:poisson}
    Lad $\lambda>0$, hvis en diskret stokastisk variabel $X$ har en pmf
    \begin{align*}
         p(k)=\e^{-\lambda}\frac{\lambda^k}{k!}\text{ for } k\in\mathbb{N}_0
    \end{align*}
    så har $X$ en \textbf{Poisson fordeling} med parameter $\lambda$, noteret ved $X\sim\Poi(\lambda)$.
\end{defn}

\begin{exmp}\label{exmp:kundeservice}
En kundeservice benytter en Poisson fordeling med $\lambda = 10$ til at modellere, hvor mange opkald de forventer på en time, så de har en ide, om hvor meget personale, de skal kalde ind på arbejde. Hver medarbejder kan varetage $6$ opkald i timen. Lad $X$ være antallet af opkald på en time, så $X \sim \Poi(10)$.
Sandsynligheden for, at der er mere end $18$ opkald, er
\begin{align*}
    P(X > 18) = 1 - P(X \leq 18) \approx 1 - 0.985 = 0.015
\end{align*}
Altså vil det i langt de fleste tilfælde være tilstrækkeligt at have 3 medarbejdere på arbejde. 
\end{exmp}
\begin{prop}\label{prop:poiForventedOgVarians} %prop 2.19
Lad $\lambda > 0$ og $X \sim \Poi(\lambda)$, så gælder
\begin{equation*}
    E[X] = \Var[X] = \lambda
\end{equation*}
\end{prop}
\begin{proof}
Noter at $kp(k) = k \e^{-\lambda} \frac{\lambda^k}{k!} = \lambda \e^{-\lambda} \frac{\lambda^{k - 1}}{(k - 1)!} = \lambda p(k - 1)$, for $k = 1, 2, \ldots$, heraf følger det, at 
\begin{equation*}
    E[X] = \sum^\infty_{k = 0} k p(k) = \sum^\infty_{k = 1} k p(k) = \lambda \sum^\infty_{k = 1} p(k - 1) = \lambda,
\end{equation*}
da $\sum^\infty_{k = 1} p(k - 1) = \sum^\infty_{k = 0} p(k) =  1$, jævnfør proposition \ref{prop:kravTilPMF}.
Variansen af $X$ beregnes ved formelen $\Var[X] = E[X^2] - E[X]^2$, jævnfør korollar \ref{cor:VariansIForholdTilForventedVærdi}. Vi ved, at $E[X]^2 = \lambda^2$, og at $E[X^2] = E[X] + E[X(X - 1)]$, jævnfør lemma \ref{lem:middelværdiAfX2}. Men $k(k - 1)p(k) = 0$ for $k = 0, 1$, heraf følger det, at
\begin{align*}
    E[X(X-1)] = \sum^{\infty}_{k = 0} k (k - 1) p(k)
    &= \sum^\infty_{k = 2} k(k - 1) p(k) \\ &=  \lambda\sum^\infty_{k = 2} (k - 1) p(k - 1) =\lambda^2 \sum^\infty_{k = 2} p(k - 2) = \lambda^2
\end{align*}
da $\sum^\infty_{k = 2} p(k - 2) = \sum^\infty_{k = 0} p(k) = 1$, jævnfør proposition \ref{prop:kravTilPMF}, det følger, at 
\begin{equation*}
    \Var[X] = E[X^2] - E[X]^2 = E[X] + E[X(X - 1)] - E[X]^2 = \lambda + \lambda^2 - \lambda^2 = \lambda
\end{equation*}
\end{proof}
