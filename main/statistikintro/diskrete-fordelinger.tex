\section{Diskrete fordelinger}
% Se martins rettelser
Lad $A$ være en hændelse i et forsøg og lad $\lambda = P(A) > 0$, som gentages uafhængigt af tidligere forsøg. Lad $X$ være antallet af forsøg, der udføres før $A$ opstår første gang. Så følger det at $X$ har udfaldsrum $\{1, 2, \ldots\}$, og at $X = k$ hvis og kun $A$ opstod i det $k$'te forsøg men ikke i de første $k - 1$ forsøg. Grundet uafhængigheden af gentagelserne af forsøgene, så er sandsynligheden for at $X = k$ givet ved
\begin{align*}
    \lambda(1-\lambda)(1-\lambda)\cdots (1-\lambda) =\lambda(1-\lambda)^{k-1}
\end{align*}
for $k \in \N$.
\begin{defn} \label{def:2.14} %Def 2.14
  Lad $A$ være en hændelse og lad $\lambda = P(A) > 0$. Lad $X$ være antallet af gentagelser før $A$ opstår, så har $X$ pmf
  \begin{align*}
    p(k)=\lambda(1-\lambda)^{k-1}, \quad k=1,2,\ldots 
  \end{align*}
  og $X$ siges at følge en \textbf{geometrisk fordeling} med parametre $\lambda$, og vi notere $X\sim\geom(\lambda)$.
\end{defn}

Hvis der i stedet for kigges på $Y$, som er antallet af forsøg, strengt før hændelsen $A$ opstår, så er
\begin{align*}
    p_{Y}(k)=\lambda(1-\lambda)^{k}, \quad k=0,1,2,\ldots
\end{align*}
hvilket svare til en geometrisk fordeling med $k$ startende i $0$. Det gælder også, at $Y=X-1$, såfremt variablerne bygger på samme forsøg og hændelse $A$.

\begin{exmp}
En terning kastes indtil, den første sekser bliver slået. Lad $A_k = \{\text{det } k \text{'te kast er en 6'er}\}$. Betragt, at $A_1, A_2, \ldots$ er uafhængige, se eksempel \ref{exmp:uafhængelighed}. Lad $X$ være antallet af terninger kastet før, den første sekser bliver slået, så er $X \sim \geom\left(\tfrac{1}{6}\right)$, da der er tale om et stokastisk forsøg, som gentages $n$ uafhængige gange. Hvad er sandsynligheden for, at den første sekser kommer ved $10.$ slag? Vi har 
\begin{equation*}
    p(10) = \frac{1}{6} \left(1-\frac{1}{6}\right)^{10 - 1} \approx 0.032.
\end{equation*}
\end{exmp}


\begin{prop} %2.18 COPY PASTE, men det er svært at lave om
\label{prop:geomEgenskaber}
Hvis $X \sim \geom (p)$, så gælder 
\begin{align*}
    E[X] = \frac{1}{p} \text{ og } \Var[X] = \frac{1-p}{p^2}
\end{align*}
\end{prop}

\begin{proof}
    Ud fra proposition \ref{prop:2.9} kan middelværdien af $X$ opskrives som
    \begin{align*}
      E[X] &= \sum_{n=0}^{\infty} P(X > n) =\sum_{n=0}^\infty(1-p)^n=\frac{1}{1-(1-p)}=\frac{1}{p}.
    \end{align*}
    hvor det vides, at $\sum_{n=0}^\infty x^n=\frac{1}{1-x}$ for alle $|x|<1$, hvor vi har, at $|p-1|<1$.\\
    Jævnfør fra korollar \ref{cor:VariansIForholdTilForventedVærdi}, at
    \begin{align*}
        \Var[X] =E[X^2]-E[X]^2
    \end{align*}    
    For at udregne $E[X^2]$, så bemærkes det først, at hvis
    \begin{align*}
        \sum^\infty_{n=0}x^n=\frac{1}{1-x}
    \end{align*}
    for alle $|x|<1$ differentieres to gange, fåes 
    \begin{align*}
        \sum^\infty_{n=2}(n(n-1))x^{n-2}=\frac{2}{(1-x)^3}
    \end{align*}
    Yderligere bemærk, at $X^2=X(X-1)+X$, hermed fåes det, at
    \begin{align*}
        E[X^2]=E[X(X-1)]+E[X]
    \end{align*}
    hvor 
    \begin{align*}
        E[X(X-1)]&= \sum_{k=1}^{\infty}k(k-1)p(1-p)^{k-1}\\
        &= p(1-p)\sum_{k=0}^{\infty}k(k-1)(1-p)^{k-2}\\
        &=p(1-p)\frac{2}{(1-(1-p))^3}
        \\
        &=(1-p)\frac{2}{p^2}
    \end{align*}
    Hermed fåes det så, at
    \begin{align*}
        \Var[X]=(1-p)\frac{2}{p^2}+\frac{1}{p}-\left(\frac{1}{p}\right)^2
        =\frac{2(1-p)+p-1}{p^2}
        =\frac{1-p}{p^2}
    \end{align*}
% \begin{align*}
%     E[X]&=\sum_{k=1}^\infty kp(1-p)^{k-1} =p\sum_{k=0}^\infty k(1-p)^{k-1}=p\frac{1}{1-(1-p)^2}=\frac{1}{p}
% \end{align*}
% Man kan opskrive $\Var[X] =E[X^2]-E[X]^2$, jævnfør korollar \ref{cor:VariansIForholdTilForventedVærdi}\\
% Ud fra lemma \ref{lem:middelværdiAfX2} er $E[X^2]$ givet ved:
% \begin{align*}
% E[X^2]&=E[X]+E[X(X-1)]\\
%     &=\frac{1}{p}+\sum_{k=1}^{\infty}k(k-1)p(1-p)^{k-1}\\
%     &=\frac{1}{p} + p(1-p)\sum_{k=0}^{\infty}k(k-1)(1-p)^{k-2}\\
%     &=\frac{1}{p}+p(1-p)\frac{2}{(1-(1-p))^3}\\
%     &=\frac{2-p}{p^2}
% \end{align*}
% og derved er:
% \begin{align*}
%      \Var[X]&=E[X^2]-E[X]^2
%      =\frac{2-p}{p^2}-\left(\frac{1}{p}\right)^2=\frac{1-p}{p^2}
% \end{align*}
\end{proof}

Et andet eksempel på en diskrete fordeling er Poissons. Denne fordeling kan benyttes til at beskrive antallet af stokastiske hændelser, over en periode. Eksempelvis antallet af jordskælv over et årti.
%Def 2.15
\begin{defn}\label{def:poisson}
    Lad $\lambda>0$, hvis $X$ har pmf
    \begin{align*}
         p(k)=\e^{-\lambda}\frac{\lambda^k}{k!}\text{ for } k\in\mathbb{N}_0
    \end{align*}
    så $X$ følger en \textbf{Poisson fordeling} med parameter $\lambda$, noteret ved $X\sim\Poi(\lambda)$.
\end{defn}

\begin{exmp}\label{exmp:kundeservice}
En kundeservice benytter en Poisson fordeling med $\lambda = 10$ til at modellere, hvor mange opkald de forventer på en time, så de har en ide om hvor meget personale, de skal kalde ind på arbejde. Hver medarbejder kan varetage $6$ opkald i timen. Lad $X$ være antallet af opkald på en time, så $X \sim \Poi(10)$.
Sandsynligheden for, at der er mere end $18$ opkald, er
\begin{align*}
    P(X > 18) = 1 - P(X \leq 18) \approx 1 - 0.985 = 0.015
\end{align*}
Altså vil det i langt de fleste tilfælde være tilstrækkeligt at have 3 medarbejdere på arbejde. 
\end{exmp}
\begin{prop}\label{prop:poiForventedOgVarians} %prop 2.19
Lad $\lambda > 0$ og $X \sim \Poi(\lambda)$, så gælder
\begin{equation*}
    E[X] = \Var[X] = \lambda
\end{equation*}
\end{prop}
\begin{proof}
Det gælder, at $kp(k) = k \e^{-\lambda} \frac{\lambda^k}{k!} = \lambda \e^{-\lambda} \frac{\lambda^{k - 1}}{(k - 1)!} = \lambda p(k - 1)$ for $k = 1, 2, \ldots$, heraf følger det, at 
\begin{equation*}
    E[X] = \sum^\infty_{k = 0} k p(k) = \sum^\infty_{k = 1} k p(k) = \lambda \sum^\infty_{k = 1} p(k - 1) = \lambda,
\end{equation*}
da $\sum^\infty_{k = 1} p(k - 1) = \sum^\infty_{k = 0} p(k) =  1$, jævnfør proposition \ref{prop:kravTilPMF}.
Variansen af $X$ beregnes ved formelen $\Var[X] = E[X^2] - E[X]^2$, jævnfør korollar \ref{cor:VariansIForholdTilForventedVærdi}. Vi ved, at $E[X]^2 = \lambda^2$, og at $E[X^2] = E[X] + E[X(X - 1)]$, jævnfør lemma \ref{lem:middelværdiAfX2}. Men $k(k - 1)p(k) = 0$ for $k = 0, 1$, heraf følger det, at
\begin{align*}
    E[X(X-1)] = \sum^{\infty}_{k = 0} k (k - 1) p(k)
    &= \sum^\infty_{k = 2} k(k - 1) p(k) \\ &=  \lambda\sum^\infty_{k = 2} (k - 1) p(k - 1) =\lambda^2 \sum^\infty_{k = 2} p(k - 2) = \lambda^2
\end{align*}
da $\sum^\infty_{k = 2} p(k - 2) = \sum^\infty_{k = 0} p(k) = 1$, jævnfør proposition \ref{prop:kravTilPMF}, det følger, at 
\begin{equation*}
    \Var[X] = E[X^2] - E[X]^2 = E[X] + E[X(X - 1)] - E[X]^2 = \lambda + \lambda^2 - \lambda^2 = \lambda
\end{equation*}
\end{proof}
